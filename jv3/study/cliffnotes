
python manage.py shell

## startup
from django.contrib.auth.models import User
from jv3.models import *
from jv3.utils import *
import jv3.study.content_analysis as ca
import jv3.study.ca_datetime as cadt
import jv3.study.ca_sigscroll as cass
import jv3.study.ca_load as cal
import jv3.study.ca_plot as cap
import jv3.study.ca_search as cas
import rpy2
import rpy2.robjects as ro
from jv3.study.study import *
from numpy import array
r = ro.r
emax = User.objects.filter(email="emax@csail.mit.edu")[0]
emax2 = User.objects.filter(email="electronic@gmail.com")[0]
brenn = User.objects.filter(email="brennanmoore@gmail.com")[0]
gv = User.objects.filter(email="gvargas@mit.edu")[0]
devoff = lambda : r('dev.off()')
c = lambda vv : apply(r.c,vv)


## consenting users 
[ u for u in User.objects.all() if is_consenting_study2(u)]

## consenting notes
n = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )

## sort top consenting users
us = [u for u in User.objects.all() if is_consenting_study2(u)]
un = dict([(u.id,u.note_owner.all().count()) for u in us])
us.sort(lambda a,b: un[b.id] - un[a.id])
us[0:10]

## sort top users

Us = [u for u in User.objects.all() if u.note_owner.all().count() > 20]
print len(Us)
un = dict([(u.id,u.note_owner.all().count()) for u in Us])
Us.sort(lambda a,b: un[b.id] - un[a.id])
Us.remove(brenn)
Us.remove(emax)
Us.remove(emax2)
Us[0:10]

## let's test to see how insane our actlogs are

def plot_actlog_times_for(u):
   uact = u.activitylog_set
   whens = [float(x['when']) for x in uact.values('when')]
   whens.sort()
   cap.scatter( [(i, whens[i]) for i in r.seq(0,len(whens)-1)], filename='/var/listit/www-ssl/_studyplots/time_seq.png')

def draw_datelines(whens):
   start = min(whens)
   end = max(whens)
   while start < end:
      datetime.datetime.fromtimestamp(start)
   # broken!


## KEEPING - inter part variation
## simply print ratio of keep:delete

def userlifespan_days(u):
   uact = u.serverlog_set
   if uact.count() == 0:  return 0         
   print ">>>"
   print uact.order_by('-when')[uact.count()-1].when, " 0 "
   print uact.order_by('when')[uact.count()-1].when
   print "<<<"
   return float(uact.order_by('when')[uact.count()-1].when - uact.order_by('-when')[uact.count()-1].when)/(24*60*60000.0)

keeping_interpart_variation=lambda Us,N: [(
u.email, ## v[0]
u.note_owner.filter(deleted=0).count(),   ## 1
u.note_owner.filter(deleted=1).count(),           ## 2
1.0*u.note_owner.filter(deleted=0).count()/(1+u.note_owner.filter(deleted=1).count()), ## 3
100.0*u.note_owner.filter(deleted=0).count()/(1+u.note_owner.filter(deleted=0).count()+u.note_owner.filter(deleted=1).count()), ## 4
userlifespan_days(u) ## 5
) for u in Us[0:N]]

keeping_interpart_variation_str=lambda Us,N=2000: '\n'.join([repr(p) for p in keeping_interpart_variation(Us,N)])

print " lifespan ", min([v[5] for v in vv]), " mean ", mean([v[5] for v in vv]), " max ", max([v[5] for v in vv])


## in order to get rid of outliers that are more than X standard deviations,
## we assume normalcy and
vv = keeping_interpart_variation(Us,10000)           
z = lambda x,v:  (x - mean(v))/( r.var(c(v))[0]**0.5 )
v_1s = [v_[1] for v_ in vv]
v_2s = [v_[2] for v_ in vv]
vv_nice = [v for v in vv if z(v[1],v_1s) < 8 and z(v[2],v_2s) < 8 and v[5] >= 0]

# draw a histogram of the percentage kept
cap.hist( c([v[4] for v in vv]),breaks=r.seq(0,100.1,by=5), filename='/var/listit/www-ssl/_studyplots/percentage_kept.png', title='percentage kept')

## draw 
max_v5 = max([v[5] for v in vv_nice])
max_radius = 15.0
cap.scatter( [(v[1],v[2]) for v in vv_nice] , filename='/var/listit/www-ssl/_studyplots/kept_v_deleted.png', title='kept vs deleted', asp=1.0, cex=c([max_radius*v[5]/max_v5 for v in vv_nice]),devoff=False)           
r.lines(r.seq(0,800))
devoff()

## histogram of usage
cap.hist( c([v[5] for v in vv_nice]),breaks=r.seq(0,max([v[5] for v in vv_nice])+31,by=7), filename='/var/listit/www-ssl/_studyplots/listit_lifetime.png', title='lifetime in days %d users (min: %g, median:%g max:%g)' % (len(vv_nice),min([v[5]for v in vv_nice]),median([v[5] for v in vv_nice]),max([v[5] for v in vv_nice])))

## sampling notes
rn = cal.random_notes(25000)

## content features
x,y = ca.notes_to_features(rn,False,ca.content_features)

## import a spreadsheet that has already been coded
renotes,cols = cal.import_notes_csv('/biggie/listit/study2010/wolfe-kp-2.5k.csv')
x,y = ca.notes_to_features(renotes,False)

## loading marked up spreadsheet

>>> notes,cols = cal.import_notes_csv('/biggie/listit/study2010/exported-2500-subset-CODED-kp-w-b.csv')
# then "notes" will have all the notes you loaded from the database as NOTES VALUES

# then "cols" will contain the actual spreadsheet; if you want only the kp TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[3] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the wolfe TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[4] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[5] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN REF VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[6] == '1' ]

## NOTEWISE revisitation curves
## consenting users 
u2 = [ u for u in User.objects.all() if is_consenting_study2(u)]
u1 = [ u for u in User.objects.all() if is_consenting_study1(u)]

## consenting notes
n1 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study1(u)] )
n2 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )
ns = [ca._note_instance_to_value(n) for n in cal.filter_notes( list(set(n1).union(n2)) ) ]
nfk,nf = ca.notes_to_features(ns,False)
## perform histogram/stats on ALL features
ca.feature_hists(nfk,nf)

cass.select_dudes_to_revisit(ns)


## show notes:

## eyeballing individual features
ns = cal.random_notes(10000)
cal._prime_actlog_cache(ns)
nfk,nf = ca.notes_to_features(ns,False)
ca.show_notes(ns,nf,nffilter=lambda nf: nf["note_words"] == 1) ## ncfilter= some fn of note properties

# verbs only
ca.show_notes(ns,nf,nffilter=lambda nf: nf["VBN"] == 0 and nf["VBN"] == 0 and nf["VBZ"] == 0 and nf["VBG"] == 0 and nf["VBD"] == 0 and nf["VBN"] == 0 and nf["VBP"] == 0 and nf["VB"] == 0)


notes = cal.random_notes(2500)

for n in notes:
   cas.user_search(n["owner"])

sc = cas.search_cache
qc = cas.search_query_cache

# old code to get % >1 word
1.0 * len([q for q in aq if len(q.split())>1]) / ( len(aq) - len([q for q in aq if len(q.split())<1]) )


uc = [ u for u in User.objects.all() if is_consenting_study2(u)]

cas.get_searches(uc)

wps = cas.words_per_search()

percent_searches_1_word = 1.0 * len([x for x in wps if x == 1]) / len(wps)

# filter eliminates that one outlier that messes up the whole graph
cap.hist([n for n in wps if n < 20],breaks=range(1,20),filename="/var/www/listit-study/words_per_search.png",title="words per search")

tr = cas.times_repeated()

trc = []
for user in tr:
  for q in tr[user]:
    trc.append(tr[user][q])

same_area = []
for num in trc:
  same_area.extend([num]*num)

1.0 * [x for x in same_area if x == 1] / len(same_area)

cap.hist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated.png",title="times query repeated")
cap.loghist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_l.png",title="times query repeated log")
cap.hist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa.png",title="times query repeated sa")
cap.loghist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa_l.png",title="times query repeated sa log")

avg_num_queries_per_user = [len(tr[u]) for u in tr]
ca.s(avg_num_queries_per_user)
min:0 max:122 mode:0 mean:2.39804 median:0 var:75.4197

avg_num_queries_per_user_greater_than_0 = [len(tr[u]) for u in tr if len(tr[u]) > 0]
ca.s(avg_num_queries_per_user_greater_than_0)
min:1 max:122 mode:1 mean:10.8088 median:5 var:250.319

qc = cas.search_query_cache

qvc = [] # number of searches / unique queries for users that search
for user in qc:
  if (len(qc[user]) > 0):
    qvc.append(1.0* len(qc[user]) / len(tr[user].keys()))

min:1 max:6 mode:1 mean:1.75747 median:1.45455 var:0.781853

aq = cas.all_queries

# data about repeated queries globally
tq = {}
for q in aq:
  if q.lower() not in tq:
    tq[q.lower()] = 1
  else:
    tq[q.lower()] += 1

tqc = [tq[q] for q in tq]

ca.s(tqc)
min:1 max:549 mode:1 mean:2.48317 median:1 var:242.346

cap.loghist(tqc,breaks=range(0,550,5),filename="/var/www/listit-study/total_queries.png",title="total queries")

[x for x in tq if tq[x] >= 20]

todo = [x for x in tq if 'todo' in x or 'to do' in x]
n = 0
for x in todo:
  n += tq[x]

n == 215
1.0 * n / len(aq)
