python manage.py shell

## startup
import os,sys
from django.contrib.auth.models import User
from jv3.models import *
from jv3.utils import *
import jv3.study.content_analysis as ca
import jv3.study.ca_datetime as cadt
import jv3.study.ca_sigscroll as cass
import jv3.study.ca_load as cal
import jv3.study.ca_plot as cap
import jv3.study.ca_search as cas
import rpy2
import rpy2.robjects as ro
from jv3.study.study import *
from numpy import array
import jv3.study.f as stuf
r = ro.r
emax = User.objects.filter(email="emax@csail.mit.edu")[0]
emax2 = User.objects.filter(email="electronic@gmail.com")[0]
brenn = User.objects.filter(email="brennanmoore@gmail.com")[0]
gv = User.objects.filter(email="gvargas@mit.edu")[0]
wstyke = User.objects.filter(email="wstyke@gmail.com")[0]
katfang = User.objects.filter(email="justacopy@gmail.com")
karger = User.objects.filter(email="karger@mit.edu")
devoff = lambda : r('dev.off()')
c = lambda vv : apply(r.c,vv)
cap.set_basedir('/var/listit/www-ssl/_studyplots/')

## consenting users 
consenting = [ u for u in User.objects.all() if is_consenting_study2(u)]
interesting_consenting = [ u for u in consenting if u.note_owner.count() > 60 ]

# participant report
excluders = [emax,emax2,brenn,gv,wstyke]

def participant_report():
   print "users "
   cu = [ u for u in User.objects.all() if is_consenting_study2(u)]
   print "total %d " % len(User.objects.all())
   print "total nonotes %d " % len([ u for u in User.objects.all() if u.note_owner.count() == 0])
   print "consenting %d " % len(cu)
   print "consenting somenotes %d " % len([c for c in cu if c.note_owner.count() > 0])
   print "consenting nonotes %d " % len([c for c in cu if c.note_owner.count() == 0])
   print "total notes %d " % Note.objects.exclude(owner__in=excluders).count()
   print "consenting notes %d " % Note.objects.filter(owner__in=cu).count()
   durations = [ float(u.note_owner.order_by('-edited')[0].edited - u.note_owner.order_by('edited')[0].edited)/(24.0*60.0*60000.0) for u in User.objects.all() if u.note_owner.count() > 0  ]   
   print 'average duration of use min:%g max:%g median:%g mean: %g:' % (min(durations),max(durations),median(durations),mean(durations))
   durations = [ float(uact.serverlog_set.order_by('when')[uact.serverlog_set.count()-1].when - uact.serverlog_set.order_by('-when')[uact.serverlog_set.count()-1].when) for uact in User.objects.all() if uact.serverlog_set.count() > 0]
   print 'average duration of use min:%g max:%g median:%g mean: %g:' % (min(durations),max(durations),median(durations),mean(durations))   
   print 'users still active in the last 14 days %d' % len(set([x['user'] for x in ServerLog.objects.order_by('when').filter(when__gt=repr(time.time()*1000 - 14*24*60*60000)).values('user')]))




[ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).count() for n in emax.note_owner.order_by('-created')[0:200]]
[ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).count() for n in emax.note_owner.order_by('-created')[0:1000]]


## import f! 
      
 distances = ([abs(float(ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).order_by('when')[0].when) - float(n.created)) for n in emax.note_owner.all() if ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).count() == 1])


max([abs(float(ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).order_by('-when')[0].when) - float(n.created)) for n in emax.note_owner.order_by('-created')[0:1000] if ActivityLog.objects.filter(owner=emax,action="note-add",noteid=n.jid).count() > 0])

## Given ONE user's notes, plot note attribute (created) vs activitylog attribute (when)
def aPlot(filename, notes,  title='title'):
   points = {'note-add':r.c(),'note-save':r.c(),'note-delete':r.c()}
   births = {}
   deaths = {}
   today = time.time()*1000.0
   
   r.png(file = '/var/listit/www-ssl/_studyplots/' + filename + '.png', w=3200,h=1600)
   allLogs = ActivityLog.objects.filter(owner=notes[0].owner, action__in=['note-add','note-save','note-delete'])
   for log in allLogs:
      noteArr = notes.filter(jid=log.noteid)
      if len(noteArr) < 1:
         continue
      note = noteArr[0]
      ## birth/deahts
      births[note.jid] = note.created
      if not note.deleted and note.jid not in deaths:
         deaths[note.jid] = today
         pass
      if (log.action == 'note-delete'):
         deaths[note.jid] = float(log.when)
      points[log.action] = r.rbind(points[log.action],c([float(note.created),float(log.when)]))
      pass
   r.plot(points['note-add'], cex=1.0,col = "green", pch='o',xlab='',ylab='')
   r.points(points['note-save'], col = "purple", pch='o',cex=3.0)
   r.points(points['note-delete'], cex=1.0,col = "dark red", pch='x')
   #r.lines( reduce(r.rbind,[r.c(float(births[x]),float(deaths[x])) for x in births.keys() if x in deaths],r.c()), col = "dark red")
   for x in births.keys():
      if x in deaths:
         if today - deaths[x] < 0.001 :
            color = 'green'
         else:
            color = 'black'
         r.lines(c([float(births[x])]*2),c([float(births[x]),float(deaths[x])]), col=color)
         pass
   devoff()


def diaginfos():
   import json
   results = []
   for slog in ServerLog.objects.filter(url='post_diagnostics').values('info'):
      try:
         info = json.loads(slog['info'])[0]
         results.append(info)
      except:
         print sys.exc_info()
   return results

def diaginfos_by_clientid(infos=None):
   import json
   if infos is None: infos = diaginfos()
   return dict((info['clientid'],info) for info in infos)

infos = diaginfos()
icb = diaginfos_by_clientid(infos)

number_of_distinct_clients = lambda icb: len(icb.keys())
number_of_distinct_users = lambda icb: len(list(set([v['username'] for v in icb.values()])))
number_of_blank_users = lambda icb: len(list([v['username'] for v in icb.values() if v['username'] == '']))

def clients_per_user(infos):
   by_user = {}
   if infos is None: infos = diaginfos()
   for info in infos :
      username = info.get('username','')
      if username == '': continue
      userclients = by_user.get(username,[])
      userclients.append(info)
      by_user[username] = userclients
   return by_user

number_of_clients_per_user = lambda infos: [len(set([X['clientid'] for X in info])) for info in clients_per_user(infos).values()]

def histogram_of_number_of_computers_per_user(width=900,height=500,filename='/var/listit/www-ssl/_studyplots/clients.png'):
   clientsperuser = number_of_clients_per_user(diaginfos())
   print clientsperuser
   #print "one"
   r.png('/tmp/foo',width=1024,height=768)
   aa = r.hist(c(clientsperuser))
   devoff();
   print "twos"
   try:
      print sum(aa[1])
      print "outputting to %s " % filename
      cap.hist(clientsperuser,filename=filename, labels=True,xlabel='number of computers per user', ylabel='number of users', width=width,height=height,title="Number of computers for signed up users (N=%d)" % sum(aa[1]))
   except:
      print sys.exc_info()
   return clientsperuser

def histogram_of_platforms(infos,filename='/var/listit/www-ssl/_studyplots/client_platforms.png'):
   platforms = {}
   infos = [v for v in diaginfos_by_clientid(infos).values()]
   for info in infos:
      platforms[info['platform_os']] = platforms.get(info['platform_os'],0) + 1
   print [x for x in platforms.iteritems()]
   cap.bar([x for x in platforms.iteritems()], width=900,height=500,filename=filename,labels=True,title="Distribution of List-It client installs by platform")
   return platforms

## consenting notes
n = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )

## sort top consenting users
us = [u for u in User.objects.all() if is_consenting_study2(u)]
un = dict([(u.id,u.note_owner.all().count()) for u in us])
us.sort(lambda a,b: un[b.id] - un[a.id])
us[0:10]

## sort top users

Us = [u for u in User.objects.all() if u.note_owner.all().count() > 20]
print len(Us)
un = dict([(u.id,u.note_owner.all().count()) for u in Us])
Us.sort(lambda a,b: un[b.id] - un[a.id])
Us.remove(brenn)
Us.remove(emax)
Us.remove(emax2)
Us[0:10]

## let's test to see how insane our actlogs are

def plot_actlog_times_for(u):
   uact = u.activitylog_set
   whens = [float(x['when']) for x in uact.values('when')]
   whens.sort()
   cap.scatter( [(i, whens[i]) for i in r.seq(0,len(whens)-1)], filename='/var/listit/www-ssl/_studyplots/time_seq.png')


## KEEPING - inter part variation
## simply print ratio of keep:delete

def userlifespan_days(u):
   uact = u.serverlog_set
   if uact.count() == 0:  return 0         
   print ">>>"
   print uact.order_by('-when')[uact.count()-1].when, " 0 "
   print uact.order_by('when')[uact.count()-1].when
   print "<<<"
   return float(uact.order_by('when')[uact.count()-1].when - uact.order_by('-when')[uact.count()-1].when)/(24*60*60000.0)

keeping_interpart_variation=lambda Us,N: [(
u.email, ## v[0]
u.note_owner.filter(deleted=0).count(),   ## 1
u.note_owner.filter(deleted=1).count(),           ## 2
1.0*u.note_owner.filter(deleted=0).count()/(1+u.note_owner.filter(deleted=1).count()), ## 3
100.0*u.note_owner.filter(deleted=0).count()/(1+u.note_owner.filter(deleted=0).count()+u.note_owner.filter(deleted=1).count()), ## 4
userlifespan_days(u) ## 5
) for u in Us[0:N]]

keeping_interpart_variation_str=lambda Us,N=2000: '\n'.join([repr(p) for p in keeping_interpart_variation(Us,N)])

print " lifespan ", min([v[5] for v in vv]), " mean ", mean([v[5] for v in vv]), " max ", max([v[5] for v in vv])

## wnotes for edit life plots 
## find . -name '*.png' -exec echo '<a href="{}" target="_blank"><img src="{}" width=400></a> &nbsp ' \; > index2.html


## in order to get rid of outliers that are more than X standard deviations,
## we assume normalcy and
vv = keeping_interpart_variation(Us,10000)           
z = lambda x,v:  (x - mean(v))/( r.var(c(v))[0]**0.5 )
v_1s = [v_[1] for v_ in vv]
v_2s = [v_[2] for v_ in vv]
vv_nice = [v for v in vv if z(v[1],v_1s) < 8 and z(v[2],v_2s) < 8 and v[5] >= 0]

# draw a histogram of the percentage kept
cap.hist( c([v[4] for v in vv]),breaks=r.seq(0,100.1,by=5), filename='/var/listit/www-ssl/_studyplots/percentage_kept.png', title='percentage kept')

## draw 
max_v5 = max([v[5] for v in vv_nice])
max_radius = 15.0
cap.scatter( [(v[1],v[2]) for v in vv_nice] , filename='/var/listit/www-ssl/_studyplots/kept_v_deleted.png', title='kept vs deleted', asp=1.0, cex=c([max_radius*v[5]/max_v5 for v in vv_nice]),devoff=False)           
r.lines(r.seq(0,800))
devoff()

## histogram of usage
cap.hist( c([v[5] for v in vv_nice]),breaks=r.seq(0,max([v[5] for v in vv_nice])+31,by=7), filename='/var/listit/www-ssl/_studyplots/listit_lifetime.png', title='lifetime in days %d users (min: %g, median:%g max:%g)' % (len(vv_nice),min([v[5]for v in vv_nice]),median([v[5] for v in vv_nice]),max([v[5] for v in vv_nice])))

## sampling notes
rn = cal.random_notes(25000)

## content features
x,y = ca.notes_to_features(rn,False,ca.content_features)
## import a spreadsheet that has already been coded
renotes,cols = cal.import_notes_csv('/biggie/listit/study2010/wolfe-kp-2.5k.csv')
x,y = ca.notes_to_features(renotes,False)

## loading marked up spreadsheet

>>> notes,cols = cal.import_notes_csv('/biggie/listit/study2010/exported-2500-subset-CODED-kp-w-b.csv')
# then "notes" will have all the notes you loaded from the database as NOTES VALUES

# then "cols" will contain the actual spreadsheet; if you want only the kp TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[3] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the wolfe TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[4] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[5] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN REF VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[6] == '1' ]

## NOTEWISE revisitation curves
## consenting users 
u2 = [ u for u in User.objects.all() if is_consenting_study2(u)]
u1 = [ u for u in User.objects.all() if is_consenting_study1(u)]

## consenting notes
n1 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study1(u)] )
n2 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )
ns = [ca._note_instance_to_value(n) for n in cal.filter_notes( list(set(n1).union(n2)) ) ]
nfk,nf = ca.notes_to_features(ns,False)
## perform histogram/stats on ALL features
ca.feature_hists(nfk,nf)


cass.select_dudes_to_revisit(ns)


## show notes:

## eyeballing individual features
ns = cal.random_notes(10000)
cal._prime_actlog_cache(ns)
nfk,nf = ca.notes_to_features(ns,False)
ca.show_notes(ns,nf,nffilter=lambda nf: nf["note_words"] == 1) ## ncfilter= some fn of note properties

# verbs only
ca.show_notes(ns,nf,nffilter=lambda nf: nf["VBN"] == 0 and nf["VBN"] == 0 and nf["VBZ"] == 0 and nf["VBG"] == 0 and nf["VBD"] == 0 and nf["VBN"] == 0 and nf["VBP"] == 0 and nf["VB"] == 0)


notes = cal.random_notes(2500)

for n in notes:
   cas.user_search(n["owner"])

sc = cas.search_cache
qc = cas.search_query_cache

# old code to get % >1 word
1.0 * len([q for q in aq if len(q.split())>1]) / ( len(aq) - len([q for q in aq if len(q.split())<1]) )


uc = [ u for u in User.objects.all() if is_consenting_study2(u)]

cas.get_searches(uc)

wps = cas.words_per_search()

percent_searches_1_word = 1.0 * len([x for x in wps if x == 1]) / len(wps)

# filter eliminates that one outlier that messes up the whole graph
cap.hist([n for n in wps if n < 20],breaks=range(1,20),filename="/var/www/listit-study/words_per_search.png",title="words per search")

tr = cas.times_repeated()

trc = []
for user in tr:
  for q in tr[user]:
    trc.append(tr[user][q])

same_area = []
for num in trc:
  same_area.extend([num]*num)

1.0 * [x for x in same_area if x == 1] / len(same_area)

cap.hist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated.png",title="times query repeated")
cap.loghist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_l.png",title="times query repeated log")
cap.hist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa.png",title="times query repeated sa")
cap.loghist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa_l.png",title="times query repeated sa log")

avg_num_queries_per_user = [len(tr[u]) for u in tr]
ca.s(avg_num_queries_per_user)
min:0 max:122 mode:0 mean:2.39804 median:0 var:75.4197

avg_num_queries_per_user_greater_than_0 = [len(tr[u]) for u in tr if len(tr[u]) > 0]
ca.s(avg_num_queries_per_user_greater_than_0)
min:1 max:122 mode:1 mean:10.8088 median:5 var:250.319

qc = cas.search_query_cache

qvc = [] # number of searches / unique queries for users that search
for user in qc:
  if (len(qc[user]) > 0):
    qvc.append(1.0* len(qc[user]) / len(tr[user].keys()))

min:1 max:6 mode:1 mean:1.75747 median:1.45455 var:0.781853

aq = cas.all_queries

# data about repeated queries globally
tq = {}
for q in aq:
  if q.lower() not in tq:
    tq[q.lower()] = 1
  else:
    tq[q.lower()] += 1

tqc = [tq[q] for q in tq]

ca.s(tqc)
min:1 max:549 mode:1 mean:2.48317 median:1 var:242.346

cap.loghist(tqc,breaks=range(0,550,5),filename="/var/www/listit-study/total_queries.png",title="total queries")

[x for x in tq if tq[x] >= 20]

todo = [x for x in tq if 'todo' in x or 'to do' in x]
n = 0
for x in todo:
  n += tq[x]

n == 215
1.0 * n / len(aq)



