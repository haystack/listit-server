
python manage.py shell

## startup
from django.contrib.auth.models import User
from jv3.models import *
from jv3.utils import *
import jv3.study.content_analysis as ca
import jv3.study.ca_datetime as cadt
import jv3.study.ca_sigscroll as cass
import jv3.study.ca_load as cal
import jv3.study.ca_plot as cap
import jv3.study.ca_search as cas
import rpy2
import rpy2.robjects as ro
from jv3.study.study import *
from numpy import array
r = ro.r
em = User.objects.filter(email="emax@csail.mit.edu")[0]
gv = User.objects.filter(email="gvargas@mit.edu")[0]
c = lambda vv : apply(r.c,vv)


## consenting users 
[ u for u in User.objects.all() if is_consenting_study2(u)]

## consenting notes
n = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )

## sort top users
us = [u for u in User.objects.all() if is_consenting_study2(u)]
un = dict([(u.id,u.note_owner.all().count()) for u in us])
us.sort(lambda a,b: un[b.id] - un[a.id])
us[0:10]

## sampling notes
rn = cal.random_notes(25000)

## content features
x,y = ca.notes_to_features(rn,False,ca.content_features)

## import a spreadsheet that has already been coded
renotes,cols = cal.import_notes_csv('/biggie/listit/study2010/wolfe-kp-2.5k.csv')
x,y = ca.notes_to_features(renotes,False)

## loading marked up spreadsheet

>>> notes,cols = cal.import_notes_csv('/biggie/listit/study2010/exported-2500-subset-CODED-kp-w-b.csv')
# then "notes" will have all the notes you loaded from the database as NOTES VALUES


# then "cols" will contain the actual spreadsheet; if you want only the kp TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[3] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the wolfe TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[4] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN TODO NOTE VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[5] == '1' ]

# then "cols" will contain the actual spreadsheet; if you want only the BRENN REF VALUES:
[ [n for n in renotes if n["id"] == int(row[0])][0] for row in cols if row[6] == '1' ]

## NOTEWISE revisitation curves
## consenting users 
u2 = [ u for u in User.objects.all() if is_consenting_study2(u)]
u1 = [ u for u in User.objects.all() if is_consenting_study1(u)]

## consenting notes
n1 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study1(u)] )
n2 = Note.objects.filter( owner__in=[ u for u in User.objects.all() if is_consenting_study2(u)] )
ns = [ca._note_instance_to_value(n) for n in cal.filter_notes( list(set(n1).union(n2)) ) ]
nfk,nf = ca.notes_to_features(ns,False)
## perform histogram/stats on ALL features
ca.feature_hists(nfk,nf)

cass.select_dudes_to_revisit(ns)


## show notes:

## eyeballing individual features
ns = cal.random_notes(10000)
cal._prime_actlog_cache(ns)
nfk,nf = ca.notes_to_features(ns,False)
ca.show_notes(ns,nf,nffilter=lambda nf: nf["note_words"] == 1) ## ncfilter= some fn of note properties

# verbs only
ca.show_notes(ns,nf,nffilter=lambda nf: nf["VBN"] == 0 and nf["VBN"] == 0 and nf["VBZ"] == 0 and nf["VBG"] == 0 and nf["VBD"] == 0 and nf["VBN"] == 0 and nf["VBP"] == 0 and nf["VB"] == 0)


notes = cal.random_notes(2500)

for n in notes:
   cas.user_search(n["owner"])

sc = cas.search_cache
qc = cas.search_query_cache

# old code to get % >1 word
1.0 * len([q for q in aq if len(q.split())>1]) / ( len(aq) - len([q for q in aq if len(q.split())<1]) )


uc = [ u for u in User.objects.all() if is_consenting_study2(u)]

cas.get_searches(uc)

wps = cas.words_per_search()

percent_searches_1_word = 1.0 * len([x for x in wps if x == 1]) / len(wps)

# filter eliminates that one outlier that messes up the whole graph
cap.hist([n for n in wps if n < 20],breaks=range(1,20),filename="/var/www/listit-study/words_per_search.png",title="words per search")

tr = cas.times_repeated()

trc = []
for user in tr:
  for q in tr[user]:
    trc.append(tr[user][q])

same_area = []
for num in trc:
  same_area.extend([num]*num)

1.0 * [x for x in same_area if x == 1] / len(same_area)

cap.hist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated.png",title="times query repeated")
cap.loghist(trc,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_l.png",title="times query repeated log")
cap.hist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa.png",title="times query repeated sa")
cap.loghist(same_area,breaks=range(0,33),filename="/var/www/listit-study/times_query_repeated_sa_l.png",title="times query repeated sa log")

avg_num_queries_per_user = [len(tr[u]) for u in tr]
ca.s(avg_num_queries_per_user)
min:0 max:122 mode:0 mean:2.39804 median:0 var:75.4197

avg_num_queries_per_user_greater_than_0 = [len(tr[u]) for u in tr if len(tr[u]) > 0]
ca.s(avg_num_queries_per_user_greater_than_0)
min:1 max:122 mode:1 mean:10.8088 median:5 var:250.319

qc = cas.search_query_cache

qvc = [] # number of searches / unique queries for users that search
for user in qc:
  if (len(qc[user]) > 0):
    qvc.append(1.0* len(qc[user]) / len(tr[user].keys()))

min:1 max:6 mode:1 mean:1.75747 median:1.45455 var:0.781853

aq = cas.all_queries

# data about repeated queries globally
tq = {}
for q in aq:
  if q.lower() not in tq:
    tq[q.lower()] = 1
  else:
    tq[q.lower()] += 1

tqc = [tq[q] for q in tq]

ca.s(tqc)
min:1 max:549 mode:1 mean:2.48317 median:1 var:242.346

cap.loghist(tqc,breaks=range(0,550,5),filename="/var/www/listit-study/total_queries.png",title="total queries")

[x for x in tq if tq[x] >= 20]

todo = [x for x in tq if 'todo' in x or 'to do' in x]
n = 0
for x in todo:
  n += tq[x]

n == 215
1.0 * n / len(aq)